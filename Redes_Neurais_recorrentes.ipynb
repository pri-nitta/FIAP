{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MFbq50exiAg6kOSWQ1lvcMtP1PJum9uT",
      "authorship_tag": "ABX9TyP69v7Wh23kMXjjLpc9UZtv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pri-nitta/FIAP_IA/blob/main/Redes_Neurais_recorrentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Escrevendo um novo capitulo como Lewis Carroll"
      ],
      "metadata": {
        "id": "7kufj1Ks02un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "visReEN308V3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('/content/drive/MyDrive/wonderland.txt', 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcyX6HRX1PO5",
        "outputId": "1d25dac5-0713-4877-83ed-e72b61cc28e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 147683 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando uma amostragem das 300 primeiras palavras\n",
        "print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoRxgUwM1ez-",
        "outputId": "45ee6554-c946-4868-9dfa-09db446da057"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER I. Down the Rabbit-Hole\r\n",
            "\r\n",
            "Alice was beginning to get very tired of sitting by her sister on the\r\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\r\n",
            "book her sister was reading, but it had no pictures or conversations in\r\n",
            "it, 'and what is the use of a book,' thought Al\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total de caracteres únicos no arquivo - não se repetem\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GterZVL1syV",
        "outputId": "91d6ee87-65ec-45a0-da2f-b303eaa29435"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vetorização do texto"
      ],
      "metadata": {
        "id": "JiVOJT_62HDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo do uso de tokenização simples com unidecode slplit\n",
        "example_texts = ['alice', 'coelho']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsLI1H4W19MY",
        "outputId": "d1f2f098-9349-4f38-ba56-6e4abbb3d829"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'l', b'i', b'c', b'e'], [b'c', b'o', b'e', b'l', b'h', b'o']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizando o StringLookup - colocar um código/ token para cada palavra\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "49U69HVs2RwH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisando os IDs unicos\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_mgeX6-2aue",
        "outputId": "a8c62584-67e8-4c27-d49c-68ac70421d9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[45, 56, 53, 47, 49], [47, 59, 49, 56, 52, 59]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se eu trazer a opção invert=True, ele traz de volta cada token\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "S2Z5xKfO2lxj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Você pode tf.strings.reduce_join para juntar os caracteres de volta em strings.\n",
        "\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWnhJ-Xw29JW",
        "outputId": "67f9bb5e-531c-4f5a-9b17-0fc08436ca4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'alice', b'coelho'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "  #receber os caracteres e fazer a conversão para texto"
      ],
      "metadata": {
        "id": "SsC7xBwE2-fG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando a lista de IDs do texto\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5heJtgz3EAW",
        "outputId": "5c2e790a-0df5-4272-f2e7-3dda5acb0915"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(147683,), dtype=int64, numpy=array([18, 23, 16, ...,  1,  2,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter o vetor de texto em um fluxo de índices de caracteres.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "Af_XCwQ43SXQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8gDeE_C3c3J",
        "outputId": "e7c1da46-8560-42d0-842d-6dd36e35f5db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C\n",
            "H\n",
            "A\n",
            "P\n",
            "T\n",
            "E\n",
            "R\n",
            " \n",
            "I\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "metadata": {
        "id": "haHrYP-o3ejJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O método batch permite converter facilmente esses caracteres individuais em sequências do tamanho desejado.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JpFHW6L3hVu",
        "outputId": "1db68ac1-db33-4930-d3dc-49b243661399"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'C' b'H' b'A' b'P' b'T' b'E' b'R' b' ' b'I' b'.' b' ' b'D' b'o' b'w'\n",
            " b'n' b' ' b't' b'h' b'e' b' ' b'R' b'a' b'b' b'b' b'i' b't' b'-' b'H'\n",
            " b'o' b'l' b'e' b'\\r' b'\\n' b'\\r' b'\\n' b'A' b'l' b'i' b'c' b'e' b' ' b'w'\n",
            " b'a' b's' b' ' b'b' b'e' b'g' b'i' b'n' b'n' b'i' b'n' b'g' b' ' b't'\n",
            " b'o' b' ' b'g' b'e' b't' b' ' b'v' b'e' b'r' b'y' b' ' b't' b'i' b'r'\n",
            " b'e' b'd' b' ' b'o' b'f' b' ' b's' b'i' b't' b't' b'i' b'n' b'g' b' '\n",
            " b'b' b'y' b' ' b'h' b'e' b'r' b' ' b's' b'i' b's' b't' b'e' b'r' b' '\n",
            " b'o' b'n' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUBJdNOU3o2m",
        "outputId": "b0d27ee0-2224-4b09-f698-88885a8c81ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'CHAPTER I. Down the Rabbit-Hole\\r\\n\\r\\nAlice was beginning to get very tired of sitting by her sister on '\n",
            "b'the\\r\\nbank, and of having nothing to do: once or twice she had peeped into the\\r\\nbook her sister was re'\n",
            "b\"ading, but it had no pictures or conversations in\\r\\nit, 'and what is the use of a book,' thought Alice\"\n",
            "b\" 'without pictures or\\r\\nconversations?'\\r\\n\\r\\nSo she was considering in her own mind (as well as she coul\"\n",
            "b'd, for the\\r\\nhot day made her feel very sleepy and stupid), whether the pleasure\\r\\nof making a daisy-ch'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1] # todos os elementos da sequência original, exceto o último\n",
        "    target_text = sequence[1:] # pega todos os elementos da sequência original, exceto o primeiro\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "yXiTbwrl3vPY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Rainha\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEWrri6p4BPp",
        "outputId": "99d85f80-1750-4541-8dae-4fbdafc7168e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['R', 'a', 'i', 'n', 'h'], ['a', 'i', 'n', 'h', 'a'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "VbhS46zV4G4p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "HcdjiBpb4K1j",
        "outputId": "a45f0074-3d53-467d-c578-14a544cf7f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'CHAPTER I. Down the Rabbit-Hole\\r\\n\\r\\nAlice was beginning to get very tired of sitting by her sister on'\n",
            "Target: b'HAPTER I. Down the Rabbit-Hole\\r\\n\\r\\nAlice was beginning to get very tired of sitting by her sister on '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes"
      ],
      "metadata": {
        "id": "Cw4dEk6iBGVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o tamanho do lote (batch size)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Define o tamanho do buffer para embaralhar o dataset\n",
        "# (O TF data é projetado para trabalhar com sequências possivelmente infinitas,\n",
        "# então ele não tenta embaralhar a sequência inteira na memória. Em vez disso,\n",
        "# ele mantém um buffer no qual embaralha os elementos).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Aplica operações no dataset:\n",
        "dataset = (\n",
        "    dataset\n",
        "    # Embaralha o dataset usando o tamanho do buffer definido\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    # Agrupa os elementos em lotes de tamanho definido (BATCH_SIZE)\n",
        "    # drop_remainder=True garante que apenas lotes completos sejam mantidos\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    # Pré-carrega os lotes para otimizar a performance de treinamento\n",
        "    # tf.data.experimental.AUTOTUNE ajusta automaticamente o pré-carregamento\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Retorna o dataset processado\n",
        "dataset"
      ],
      "metadata": {
        "id": "dOWhUCha4NVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e721ad6-5a68-4676-9fcf-fbe3d1e9aa9d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construir o modelo de rede neural"
      ],
      "metadata": {
        "id": "ApNlt-x3BgKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamanho do vocabulário\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Tamanho da dimensão de embedding\n",
        "embedding_dim = 256\n",
        "\n",
        "# Número de memórias RNN\n",
        "rnn_units = 1024\n",
        "\n",
        "#logit = vai ver a probabilidade do proximo caractere ser o certo."
      ],
      "metadata": {
        "id": "OJup7gsxBf8w"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da classe do modelo, que herda de tf.keras.Model\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super(MyModel, self).__init__()\n",
        "    # Camada de embedding que converte IDs de palavras em vetores densos de dimensão embedding_dim\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    # Camada GRU com o número de unidades especificado, retornando sequências completas e estado final\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    # Camada densa que transforma as saídas da GRU em logits para cada palavra no vocabulário\n",
        "    # As saídas da GRU (Gated Recurrent Unit) em logits referem-se aos valores numéricos não normalizados produzidos pela camada densa após a GRU.\n",
        "    # Esses valores representam a pontuação ou a \"logit\" para cada classe do vocabulário, antes de serem passados por uma função de ativação, como softmax,\n",
        "    # para converter essas pontuações em probabilidades.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # Definição do método call, que especifica como os dados passam pelo modelo\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    # Entrada inicial\n",
        "    x = inputs\n",
        "    # Aplicação da camada de embedding às entradas\n",
        "    x = self.embedding(x, training=training)\n",
        "    # Se nenhum estado inicial for fornecido, inicializa o estado da GRU\n",
        "    if states is None:\n",
        "      states = [tf.zeros((x.shape[0], self.gru.units))]\n",
        "    # Passa os embeddings pela GRU, obtendo as saídas e o novo estado\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    # Passa as saídas da GRU pela camada densa para obter logits\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # Se return_state for True, retorna as saídas e o estado; caso contrário, retorna apenas as saídas\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "# Instanciação do modelo MyModel com os parâmetros fornecidos\n",
        "model = MyModel(\n",
        "    # Certifique-se de que o tamanho do vocabulário corresponde ao das camadas StringLookup\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),  # Tamanho do vocabulário\n",
        "    embedding_dim=embedding_dim,                      # Dimensão dos embeddings\n",
        "    rnn_units=rnn_units)                              # Número de unidades da GRU"
      ],
      "metadata": {
        "id": "ptA6e77iBnJ0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-R4kctyhcLH",
        "outputId": "8263f845-c009-4fbe-c95e-1da0954560db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 71) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "BKUGASquhhal",
        "outputId": "75c559f2-0a5e-4005-9cfb-3dcef4d2e943"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"my_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m18,176\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m,      │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "│                                      │ \u001b[38;5;34m1024\u001b[0m))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m71\u001b[0m)               │          \u001b[38;5;34m72,775\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,176</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">72,775</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,029,255\u001b[0m (15.37 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,029,255</span> (15.37 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,029,255\u001b[0m (15.37 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,029,255</span> (15.37 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "nNquYrKrhiV3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CfCtrsdhlcZ",
        "outputId": "4f83fff3-8cb6-44e8-bdc7-1c20f48f3013"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([45, 36, 65, 27, 70, 10, 37, 68,  0, 57, 43, 67, 22, 48,  3, 54, 16,\n",
              "        3, 60, 36, 19, 45,  9, 19, 54, 21, 32, 15, 40,  4, 40, 54, 56, 42,\n",
              "       35,  2, 66, 17, 41, 34, 10, 27, 52, 44, 60, 38, 68, 65, 58,  9, 28,\n",
              "       38, 17, 35, 69, 35, 49,  5, 58,  2, 30, 45, 14,  6, 48, 25, 33, 56,\n",
              "       57, 42, 37, 66, 51, 33, 62, 52, 42, 55, 66, 53, 29, 27, 18, 22, 55,\n",
              "       13, 20, 54, 27, 30, 26, 30,  1, 59, 55, 65, 57, 13, 27, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeOKZXoEhpi9",
        "outputId": "34efa8a0-d427-4fe5-82cd-0d788eea5dfb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'toffee, and hot\\r\\nbuttered toast,) she very soon finished it off.\\r\\n\\r\\n  *    *    *    *    *    *    '\n",
            "\n",
            "Next Char Predictions:\n",
            " b'aUuLz,Vx[UNK]m]wGd jA pUDa*DjFQ?Y!Yjl[T\\rvBZS,Lh_pWxun*MWBTyTe\"n\\rOa;\\'dJRlm[VvgRrh[kviNLCGk:EjLOKO\\nokum:Lf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento"
      ],
      "metadata": {
        "id": "O8J_n4oMjiP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "#pegar as probabilidades e utilizando multiclasses"
      ],
      "metadata": {
        "id": "SmnZzHfhhsEw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKpMc_mgjll8",
        "outputId": "c490a7b5-89a4-4500-faf3-0fcc9b5b7aa9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 71)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.262016, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS_DIOU7juFo",
        "outputId": "efd05c7d-499b-417a-b0cc-1f49b8125e70"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70.952866"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "a6sKEQnejyXL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diretório onde o checkpoint será salvo\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "5kXGNKNGj1We"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "_ZyOv0Zrj7vS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na2177OkkAoS",
        "outputId": "f03663f6-6715-4aee-d3b0-16d0f48e9874"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - loss: 4.2889\n",
            "Epoch 2/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 6s/step - loss: 2.8433\n",
            "Epoch 3/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - loss: 2.4620\n",
            "Epoch 4/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 6s/step - loss: 2.2758\n",
            "Epoch 5/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - loss: 2.1346\n",
            "Epoch 6/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - loss: 2.0251\n",
            "Epoch 7/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 6s/step - loss: 1.9273\n",
            "Epoch 8/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 6s/step - loss: 1.8305\n",
            "Epoch 9/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 6s/step - loss: 1.7487\n",
            "Epoch 10/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - loss: 1.6686\n",
            "Epoch 11/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 6s/step - loss: 1.6042\n",
            "Epoch 12/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 5s/step - loss: 1.5383\n",
            "Epoch 13/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 5s/step - loss: 1.4597\n",
            "Epoch 14/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 5s/step - loss: 1.4057\n",
            "Epoch 15/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 5s/step - loss: 1.3553\n",
            "Epoch 16/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 5s/step - loss: 1.2958\n",
            "Epoch 17/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 5s/step - loss: 1.2472\n",
            "Epoch 18/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 5s/step - loss: 1.1991\n",
            "Epoch 19/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - loss: 1.1544\n",
            "Epoch 20/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 6s/step - loss: 1.1025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    # Chama o construtor da classe base\n",
        "    super().__init__()\n",
        "    # Define a temperatura para ajuste da aleatoriedade da geração\n",
        "    # temperature: um parâmetro opcional para ajustar a aleatoriedade da geração (padrão é 1.0)\n",
        "    # quanto maior a temperatura maior a criatividade/ alucinação\n",
        "    self.temperature = temperature\n",
        "    # Define o modelo subjacente a ser usado para previsão\n",
        "    self.model = model\n",
        "    # Mapeamento de IDs para caracteres\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    # Mapeamento de caracteres para IDs\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Cria uma máscara para impedir a geração do token \"[UNK]\"\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Coloca -inf em cada índice a ser evitado\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Ajusta a forma ao vocabulário\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    # Converte a máscara esparsa em uma densa\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Converte strings em IDs de tokens\n",
        "    # chars_from_ids: uma função que mapeia IDs para caracteres\n",
        "    # ids_from_chars: uma função que mapeia caracteres para IDs\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Executa o modelo\n",
        "    # predicted_logits.shape é [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Usa apenas a última previsão\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    # Ajusta os logits pela temperatura\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Aplica a máscara de previsão para evitar a geração de \"[UNK]\"\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Amostra os logits de saída para gerar IDs de tokens\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Converte de IDs de tokens para caracteres\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Retorna os caracteres e o estado do modelo\n",
        "    return predicted_chars, states\n",
        "\n",
        "# Exemplo de instância da classe OneStep:\n",
        "# model: um modelo de previsão de texto (por exemplo, um modelo GRU ou LSTM treinado)\n",
        "\n",
        "#\n",
        "# one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=1.0)"
      ],
      "metadata": {
        "id": "2P7v_A1dkC0z"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "LZHw2NcUnOwC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time() # Inicia o cronômetro para medir o tempo de execução\n",
        "states = None # Inicializa o estado da RNN como None\n",
        "\n",
        "next_char = tf.constant(['Alice:']) # Define a string inicial para a geração de texto\n",
        "\n",
        "result = [next_char] # Cria uma lista para armazenar os caracteres gerados\n",
        "\n",
        "# Loop para gerar 1000 caracteres\n",
        "for n in range(1000):\n",
        "  # Gera o próximo caractere e o novo estado do modelo\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  # Adiciona o caractere gerado à lista de resultados\n",
        "  result.append(next_char)\n",
        "\n",
        "# Junta todos os caracteres gerados em uma única string\n",
        "result = tf.strings.join(result)\n",
        "# Para o cronômetro e calcula o tempo de execução\n",
        "end = time.time()\n",
        "\n",
        "# Imprime o texto gerado\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "\n",
        "# Imprime o tempo de execução\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1sv4UonSlF",
        "outputId": "4f3a664d-ea55-4e0c-e48b-a86700fe02c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice: (here when she was a pentried\r\n",
            "quite a surrily next quite about it, how the Gryphoon on tire when ever spoke.\r\n",
            "\r\n",
            "However, out when the Duchess seemed to raim nothing, half long live a tho wime she callled out that she did slace-began.\r\n",
            "\r\n",
            "The Doghos out to see. Picause!\r\n",
            " Which said, with one, even by the Mock Turtle, seching he\r\n",
            "the mome first were shrieed woblain the morel. 'Bale one's it exturce is, that'se whitine plas\r\n",
            "to have the nexp! Of cardle, your Majesty,' said Alice.\r\n",
            "\r\n",
            "'Will the tunn't eagerly, of 'Wust,' said the other, now the shouted to spee_ she gound tho\r\n",
            "removed it her upeapitt it wasn't yet trit the QUeen, it said and went on sitter.\r\n",
            "\r\n",
            "Alice thought, pushing a vury like thist!' wenteded the raote, when seem to have just\r\n",
            "as she call in a dance of the cauers. It was the Hatter was sighing it, and tull the next\r\n",
            "day the beauth it hard: the Mouse shreled this\r\n",
            "miguted on its mean remarked, 'What do showing\r\n",
            "the Mock Turt ears touse of pelf. I should thing,' she she he \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.9544193744659424\n"
          ]
        }
      ]
    }
  ]
}